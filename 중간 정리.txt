벡터

기저벡터

matplotlib.pyplot

ndim

transpose

squeeze

expand_dims

concatenate

선형회귀 (Linear Regression)
2진 분류
다중 분류

오차

cost function(loss function)
sklearn.linear_model

경사 하강법

fit()
predict()
hat값

a.fitter.coef_
a.fitter.intercept_

퍼셉트론 :알고리즘
신호

단순 논리 회로

다중퍼셉트론(나온이유도)
은닉층

교차 검증

홀드아웃
k겹 교차 검증
리브 p 아웃 교차검증
리브 원 아웃 교차검증
(오래걸림)

정확도 : 맞은 값/ 전체 값
무조건 정확하지 않음

Precision : 맞은 참 / (맞은 참 + 틀린 참): 즉 참이라고 예측한 모든것
Recall : 맞은 참 / (맞은 참 + 틀린 거짓):즉 원래 참인 값

sklearn모듈에 accuracy_score : 정확도를 구함

confusion_matrix 

과적합(Overfit)

Logistic Regression
한가지 선형이 아닌 것으로 분류

선형 회귀는 예상보다 큰 값이 나오면 기기준이 변화, 그래서 오차 확률증가.

시그모이드 함수
이 함수를 통해 데이터가 구분되는 시점을 분리해서 데이터가 무엇인지 분류.

Cross Entropy Loss
로지스틱 모델의 예측분포는 sigmoid(wx+b)이고, y=0일땐 앞쪽 term을 계산하
고, y=1일땐, 뒷쪽 term을 계산 loss 함수

선형 회귀는 MSE라는 LOSS함수



선형 회귀 : 예측
Logistic Regression : 2진분류

여기까지 sklearn


Keras : sklearn과 비슷한 모듈. 신경망 학습에 사용


model_fit = model.fit(x_train,y_train,batch_size=3,epochs=100,verbose=2)

model = Sequential()
model.add(Dense(32, input_dim=2, activation='sigmoid'))
model.add(Dense(1, activation='sigmoid'))

여기에서 처음 model.add의 32개는 은닉층 부분이다.

여기서는 0과 1로 구분되기 때문에 활성화 함수 시그모이드를 한 것

레이어를 많이 늘린다고 꼭 정확해 지는것은 아님

너무 많으면 경사하강법에서 반대로 튈 수 있기 때문.

적절한 레이어가 필요.


경사하강법의 문제점. : 항상 2차방정식이 아니다.

낮아졌다 높아졋다 더 낮은 경우도 생김

배치경사하강법

확률적경사하강법

미니배치경사하강법

다층 퍼셉트론
은닉층
심층신경망

딥러닝 : 이로 만들어진 모델


역전파

활성화 함수
역전파 알고리즘이 잘 동작하기 위해 다층 퍼셉트론의 구조에 변화.
계단 함수를 다른 함수로 변환

다중 분류 : softmax
1. 여러선을 이용하여 나눈다.
이것을 이용하면
카라스의 softmax란 것이 있다.
결과값의 각각의 확률을 구한것이 softmax 이다
그래서 softmax값이 가장 큰 것이 그 라벨에 속한다 라고 얘기한다.
karas.layer.Dense(10,activation = "softmax")
출력 10개


가중치 초기화
입력 안할시 랜덤,'RandomNormal'

weight값을 주는것.
'he_normal' :HE, relu활성화 함수와 사용시 성능 매우좋음.
'glorot_uniform' : Xaiver이다. 시그모이드나 tanh활성함수랑 사용.

Dropout : 일부 뉴런을 뺌
레이어 다음에 사용.


Batch Normalization
각각의 feature들의 평균과 분포를 0과1로 정규화
레이어가 깊어 질수록  값들이 점점 찌그러진 그래프를 가지게 될 수 있는데 이를 정규 분포로 만들어 주기 위해.


Fully connected layer :1차원으로 펴서 연산

Convolution layer :합성곱

필터 연산

필터, 채널

아웃풋 채널의 개수는 convolution fillter의 개수와 같다


padding
필터를 지날수록 크기가 줄어든다.(Valid)
(same)
zero padding
 
convolution은 fillter를 여러개 중첩시킬 수 있다. 그런데 너무 많은 필터와 레이어를 하면 이것 역시 오버피팅이 된다. 
 pooling
Activation map을 줄인다.

max pooling
average pooling

